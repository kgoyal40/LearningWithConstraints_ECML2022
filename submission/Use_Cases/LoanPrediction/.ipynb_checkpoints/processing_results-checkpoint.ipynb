{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "folders = list(range(1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_sbr = pd.DataFrame()\n",
    "for f in folders:\n",
    "    baseline_results = pd.read_csv(str(f) + '/all_baseline_outputs.csv').iloc[:, 1:]\n",
    "    baseline_results = baseline_results[(baseline_results['loss_type'] == 'sbr') & (baseline_results['margin/alpha'] != 'PP')]\n",
    "    alphas = [0, 0.1, 0.5, 1, 2, 5, 10, 100, 500, 1000, 10000]\n",
    "    \n",
    "    processed_baseline_results = pd.DataFrame()\n",
    "\n",
    "    for a in alphas:\n",
    "        baseline_results_sub = baseline_results[baseline_results['margin/alpha'] == str(a)]\n",
    "        mean_accuracy_test = statistics.mean(list(baseline_results_sub['accuracy_test_corrected']))\n",
    "        std_accuracy_test = statistics.stdev(list(baseline_results_sub['accuracy_test_corrected']))\n",
    "        mean_modified_accuracy_test = statistics.mean(list(baseline_results_sub['accuracy_test_modified']))\n",
    "        std_modified_accuracy_test = statistics.stdev(list(baseline_results_sub['accuracy_test_modified']))\n",
    "\n",
    "        mean_val_violations = statistics.mean(list(baseline_results_sub['violations_test_prediction']))\n",
    "        std_val_violations = statistics.stdev(list(baseline_results_sub['violations_test_prediction']))\n",
    "        mean_test_violations = statistics.mean(list(baseline_results_sub['violations_val_prediction']))\n",
    "        std_test_violations = statistics.stdev(list(baseline_results_sub['violations_val_prediction']))\n",
    "        mean_counter_examples = statistics.mean(list(baseline_results_sub['counter_examples_found']))\n",
    "        mean_runtime = statistics.mean(list(baseline_results_sub['runtime']))\n",
    "        std_runtime = statistics.stdev(list(baseline_results_sub['runtime']))\n",
    "\n",
    "        processed_baseline_results = processed_baseline_results.append([[a, mean_accuracy_test, std_accuracy_test,\n",
    "                                                                         mean_modified_accuracy_test, std_modified_accuracy_test,\n",
    "                                                                         mean_val_violations, std_val_violations,\n",
    "                                                                         mean_test_violations, std_test_violations,\n",
    "                                                                         mean_counter_examples,\n",
    "                                                                         mean_runtime, std_runtime, ]])\n",
    "    \n",
    "    processed_baseline_results.columns = ['alpha', 'average_test_accuracy', 'std_test_accuracy',\n",
    "                                          'average_test_modified_accuracy', 'std_test_modified_accuracy',\n",
    "                                          'mean val violations', 'stddev val violations',\n",
    "                                         'mean test violations', 'stddev test violations',\n",
    "                                          'mean_CE', 'mean_runtime', 'std_runtime']\n",
    "    processed_baseline_results.reset_index(inplace=True, drop=True)\n",
    "    processed_baseline_results_sub = processed_baseline_results[processed_baseline_results['mean val violations'] == 0]\n",
    "    processed_baseline_results_sub.reset_index(inplace=True, drop=True)\n",
    "    selected_alpha = processed_baseline_results_sub['alpha'][list(processed_baseline_results_sub['mean_CE']).index(min(list(processed_baseline_results_sub['mean_CE'])))]\n",
    "    print(processed_baseline_results)\n",
    "    print(selected_alpha)\n",
    "\n",
    "    out_sbr = out_sbr.append([[f, processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['average_test_accuracy'].iloc[0],\n",
    "                          processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_test_accuracy'].iloc[0],\n",
    "                             processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['average_test_modified_accuracy'].iloc[0],\n",
    "                          processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_test_modified_accuracy'].iloc[0],\n",
    "                           processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['mean_runtime'].iloc[0],\n",
    "                           processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_runtime'].iloc[0]]])\n",
    "    \n",
    "out_sbr.columns = ['experiment', 'average_test_accuracy', 'stdev_test_accuracy', 'average_test_modified_accuracy', 'std_test_modified_accuracy', 'mean_runtime', 'std_runtime']\n",
    "print(out_sbr)\n",
    "print(statistics.mean(out_sbr['average_test_accuracy']))\n",
    "print(statistics.mean(out_sbr['stdev_test_accuracy']))\n",
    "print(statistics.mean(out_sbr['average_test_modified_accuracy']))\n",
    "print(statistics.mean(out_sbr['std_test_modified_accuracy']))\n",
    "print(statistics.mean(out_sbr['mean_runtime']))\n",
    "print(statistics.mean(out_sbr['std_runtime']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_sl = pd.DataFrame()\n",
    "for f in folders:\n",
    "    baseline_results = pd.read_csv(str(f) + '/all_baseline_outputs.csv').iloc[:, 1:]\n",
    "    baseline_results = baseline_results[(baseline_results['loss_type'] == 'sl') & (baseline_results['margin/alpha'] != 'PP')]\n",
    "    alphas = [0, 0.1, 0.5, 1, 2, 5, 10, 100, 500, 1000, 10000]\n",
    "    \n",
    "    processed_baseline_results = pd.DataFrame()\n",
    "\n",
    "    for a in alphas:\n",
    "        baseline_results_sub = baseline_results[baseline_results['margin/alpha'] == str(a)]\n",
    "        mean_accuracy_test = statistics.mean(list(baseline_results_sub['accuracy_test_corrected']))\n",
    "        std_accuracy_test = statistics.stdev(list(baseline_results_sub['accuracy_test_corrected']))\n",
    "        mean_modified_accuracy_test = statistics.mean(list(baseline_results_sub['accuracy_test_modified']))\n",
    "        std_modified_accuracy_test = statistics.stdev(list(baseline_results_sub['accuracy_test_modified']))\n",
    "\n",
    "        mean_val_violations = statistics.mean(list(baseline_results_sub['violations_test_prediction']))\n",
    "        std_val_violations = statistics.stdev(list(baseline_results_sub['violations_test_prediction']))\n",
    "        mean_test_violations = statistics.mean(list(baseline_results_sub['violations_val_prediction']))\n",
    "        std_test_violations = statistics.stdev(list(baseline_results_sub['violations_val_prediction']))\n",
    "        mean_counter_examples = statistics.mean(list(baseline_results_sub['counter_examples_found']))\n",
    "        mean_runtime = statistics.mean(list(baseline_results_sub['runtime']))\n",
    "        std_runtime = statistics.stdev(list(baseline_results_sub['runtime']))\n",
    "\n",
    "        processed_baseline_results = processed_baseline_results.append([[a, mean_accuracy_test, std_accuracy_test,\n",
    "                                                                         mean_modified_accuracy_test, std_modified_accuracy_test,\n",
    "                                                                         mean_val_violations, std_val_violations,\n",
    "                                                                         mean_test_violations, std_test_violations,\n",
    "                                                                         mean_counter_examples,\n",
    "                                                                         mean_runtime, std_runtime, ]])\n",
    "    \n",
    "    processed_baseline_results.columns = ['alpha', 'average_test_accuracy', 'std_test_accuracy',\n",
    "                                          'average_test_modified_accuracy', 'std_test_modified_accuracy',\n",
    "                                          'mean val violations', 'stddev val violations',\n",
    "                                         'mean test violations', 'stddev test violations',\n",
    "                                          'mean_CE', 'mean_runtime', 'std_runtime']\n",
    "    processed_baseline_results.reset_index(inplace=True, drop=True)\n",
    "    processed_baseline_results_sub = processed_baseline_results[processed_baseline_results['mean val violations'] == 0]\n",
    "    processed_baseline_results_sub.reset_index(inplace=True, drop=True)\n",
    "    selected_alpha = processed_baseline_results_sub['alpha'][list(processed_baseline_results_sub['mean_CE']).index(min(list(processed_baseline_results_sub['mean_CE'])))]\n",
    "    print(processed_baseline_results)\n",
    "    print(selected_alpha)\n",
    "\n",
    "    out_sl = out_sl.append([[f, processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['average_test_accuracy'].iloc[0],\n",
    "                          processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_test_accuracy'].iloc[0],\n",
    "                             processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['average_test_modified_accuracy'].iloc[0],\n",
    "                          processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_test_modified_accuracy'].iloc[0],\n",
    "                           processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['mean_runtime'].iloc[0],\n",
    "                           processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_runtime'].iloc[0]]])\n",
    "    \n",
    "out_sl.columns = ['experiment', 'average_test_accuracy', 'stdev_test_accuracy', 'average_test_modified_accuracy', 'std_test_modified_accuracy', 'mean_runtime', 'std_runtime']\n",
    "print(out_sl)\n",
    "print(statistics.mean(out_sl['average_test_accuracy']))\n",
    "print(statistics.mean(out_sl['stdev_test_accuracy']))\n",
    "print(statistics.mean(out_sl['average_test_modified_accuracy']))\n",
    "print(statistics.mean(out_sl['std_test_modified_accuracy']))\n",
    "print(statistics.mean(out_sl['mean_runtime']))\n",
    "print(statistics.mean(out_sl['std_runtime']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_p = pd.DataFrame()\n",
    "for f in folders:\n",
    "    baseline_results = pd.read_csv(str(f) + '/all_baseline_outputs.csv').iloc[:, 1:]\n",
    "    baseline_results = baseline_results[(baseline_results['loss_type'] == 'sl') & (baseline_results['margin/alpha'] == 'PP')]\n",
    "    alphas = ['PP']\n",
    "    \n",
    "    processed_baseline_results = pd.DataFrame()\n",
    "\n",
    "    for a in alphas:\n",
    "        baseline_results_sub = baseline_results[baseline_results['margin/alpha'] == str(a)]\n",
    "        mean_accuracy_test = statistics.mean(list(baseline_results_sub['accuracy_test_corrected']))\n",
    "        std_accuracy_test = statistics.stdev(list(baseline_results_sub['accuracy_test_corrected']))\n",
    "        mean_modified_accuracy_test = statistics.mean(list(baseline_results_sub['accuracy_test_modified']))\n",
    "        std_modified_accuracy_test = statistics.stdev(list(baseline_results_sub['accuracy_test_modified']))\n",
    "\n",
    "        mean_val_violations = statistics.mean(list(baseline_results_sub['violations_test_prediction']))\n",
    "        std_val_violations = statistics.stdev(list(baseline_results_sub['violations_test_prediction']))\n",
    "        mean_test_violations = statistics.mean(list(baseline_results_sub['violations_val_prediction']))\n",
    "        std_test_violations = statistics.stdev(list(baseline_results_sub['violations_val_prediction']))\n",
    "        mean_counter_examples = statistics.mean(list(baseline_results_sub['counter_examples_found']))\n",
    "        mean_runtime = statistics.mean(list(baseline_results_sub['runtime']))\n",
    "        std_runtime = statistics.stdev(list(baseline_results_sub['runtime']))\n",
    "\n",
    "        processed_baseline_results = processed_baseline_results.append([[a, mean_accuracy_test, std_accuracy_test,\n",
    "                                                                         mean_modified_accuracy_test, std_modified_accuracy_test,\n",
    "                                                                         mean_val_violations, std_val_violations,\n",
    "                                                                         mean_test_violations, std_test_violations,\n",
    "                                                                         mean_counter_examples,\n",
    "                                                                         mean_runtime, std_runtime, ]])\n",
    "    \n",
    "    processed_baseline_results.columns = ['alpha', 'average_test_accuracy', 'std_test_accuracy',\n",
    "                                          'average_test_modified_accuracy', 'std_test_modified_accuracy',\n",
    "                                          'mean val violations', 'stddev val violations',\n",
    "                                         'mean test violations', 'stddev test violations',\n",
    "                                          'mean_CE', 'mean_runtime', 'std_runtime']\n",
    "    processed_baseline_results.reset_index(inplace=True, drop=True)\n",
    "    processed_baseline_results_sub = processed_baseline_results[processed_baseline_results['mean val violations'] == 0]\n",
    "    processed_baseline_results_sub.reset_index(inplace=True, drop=True)\n",
    "    selected_alpha = processed_baseline_results_sub['alpha'][list(processed_baseline_results_sub['mean_CE']).index(min(list(processed_baseline_results_sub['mean_CE'])))]\n",
    "    print(processed_baseline_results)\n",
    "    print(selected_alpha)\n",
    "\n",
    "    out_p = out_p.append([[f, processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['average_test_accuracy'].iloc[0],\n",
    "                          processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_test_accuracy'].iloc[0],\n",
    "                             processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['average_test_modified_accuracy'].iloc[0],\n",
    "                          processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_test_modified_accuracy'].iloc[0],\n",
    "                           processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['mean_runtime'].iloc[0],\n",
    "                           processed_baseline_results[processed_baseline_results['alpha'] == selected_alpha]['std_runtime'].iloc[0]]])\n",
    "    \n",
    "out_p.columns = ['experiment', 'average_test_accuracy', 'stdev_test_accuracy', 'average_test_modified_accuracy', 'std_test_modified_accuracy', 'mean_runtime', 'std_runtime']\n",
    "print(out_p)\n",
    "print(statistics.mean(out_p['average_test_accuracy']))\n",
    "print(statistics.mean(out_p['stdev_test_accuracy']))\n",
    "print(statistics.mean(out_p['average_test_modified_accuracy']))\n",
    "print(statistics.mean(out_p['std_test_modified_accuracy']))\n",
    "print(statistics.mean(out_p['mean_runtime']))\n",
    "print(statistics.mean(out_p['std_runtime']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_l = pd.DataFrame()\n",
    "for f in folders:\n",
    "    results = pd.read_csv(str(f) + '/all_outputs_sade_l.csv').iloc[:, 1:]\n",
    "    print(results)\n",
    "    mean_runtime = statistics.mean([float(r) for r in results['runtime']])\n",
    "    std_runtime = statistics.stdev([float(r) for r in results['runtime']])\n",
    "    out_l = out_l.append([[f, statistics.mean(list(results['accuracy_test_corrected'])), \n",
    "                           statistics.stdev(list(results['accuracy_test_corrected'])),\n",
    "                           statistics.mean(list(results['accuracy_test_modified'])), \n",
    "                           statistics.stdev(list(results['accuracy_test_modified'])), \n",
    "                           mean_runtime, std_runtime]])\n",
    "out_l.columns = ['experiment', 'average_test_accuracy', 'std_test_accuracy', 'average_accuracy_test_modified', 'std_accuracy_test_modified', 'mean_runtime', 'std_runtime']\n",
    "print(out_l)\n",
    "print(statistics.mean(out_l['average_test_accuracy']))\n",
    "print(statistics.mean(out_l['std_test_accuracy']))\n",
    "print(statistics.mean(out_l['average_accuracy_test_modified']))\n",
    "print(statistics.mean(out_l['std_accuracy_test_modified']))\n",
    "\n",
    "print(statistics.mean(out_l['mean_runtime']))\n",
    "print(statistics.mean(out_l['std_runtime']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count the number of violations in the data\n",
    "\n",
    "from Supplements import *\n",
    "dataset = 'loan_data_set.csv'\n",
    "data = pd.read_csv(dataset, header=0)\n",
    "\n",
    "processed_output = data_processing(data)\n",
    "data = processed_output[0]\n",
    "X = data.iloc[:, 0:data.shape[1] - 1]\n",
    "y_original = list(data.iloc[:, -1])\n",
    "scaled_income = processed_output[1].scale_[0] * (5000 - processed_output[1].data_min_[0])\n",
    "\n",
    "X, y = data_manipulation(X, y_original)\n",
    "count_violations(X, y, scaled_income)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for f in folders:\n",
    "    with (open(str(f) + \"/all_test_losses_sade_l.pkl\", \"rb\")) as openfile:\n",
    "        out = pickle.load(openfile)\n",
    "    out = [[float(i) for i in l] for l in out]\n",
    "\n",
    "    folds = 5\n",
    "    \n",
    "    plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "    plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "    plt.rcParams[\"axes.titleweight\"] = \"bold\"\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.rc('figure', figsize=(10, 5))\n",
    "\n",
    "    for e in range(folds):\n",
    "        plt.figure()\n",
    "        plt.plot(out[e])\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loan Approval (experiment = {}, fold = {})'.format(f, e + 1))\n",
    "        name = str(f) + '/loan_data_{}.jpeg'.format(e+1)\n",
    "        plt.savefig('{}'.format(name))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
